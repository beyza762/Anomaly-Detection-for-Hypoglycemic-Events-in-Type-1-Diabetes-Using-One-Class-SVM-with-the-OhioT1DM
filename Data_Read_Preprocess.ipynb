{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4cb190-15ee-4a23-af71-d3d499003eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7516f1-2484-4b36-8821-f837281ba15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "097c220a-f264-44f2-9c8b-ded60a50eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137ac19a-7cbe-4c9a-91fd-8faaca58af93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the ZIP file\n",
    "zip_file_path = r\"C:\\Praktikum AI Med\\OhioT1DM.zip\"\n",
    "\n",
    "# Specify the directory to extract the contents of the ZIP file\n",
    "extract_dir = r\"C:\\Praktikum AI Med\"\n",
    "\n",
    "# Password for encrypted files\n",
    "password = \"...@@@!!==/\\/\\/\\/\\~~~~~BGLP-1804-BGLP~~~~~/\\/\\/\\/\\==!!@@@...\"\n",
    "\n",
    "# Create the extraction directory if it doesn't exist\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Extract the contents of the ZIP file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    for member in zip_ref.infolist():\n",
    "        # Check if the file is encrypted\n",
    "        if member.flag_bits & 0x01:\n",
    "            zip_ref.extract(member, extract_dir, pwd=password.encode())\n",
    "        else:\n",
    "            zip_ref.extract(member, extract_dir)\n",
    "\n",
    "print(\"Extraction completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "164765fd-0d65-4512-b490-7a86a3959471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glc(root):\n",
    "    glucose = []\n",
    "    glucose_ts = []\n",
    "    for type_tag in root.findall('glucose_level/event'):\n",
    "        value = type_tag.get('value')\n",
    "        ts = type_tag.get('ts')\n",
    "        ts = datetime.datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "        glucose.append(int(value))\n",
    "        glucose_ts.append(ts)\n",
    "        \n",
    "    glc_frame = [glucose_ts, glucose]\n",
    "    glc_frame = np.array(glc_frame)\n",
    "    df_glc = pd.DataFrame(glc_frame.T, columns=['ts', 'glucose'])\n",
    "    return df_glc\n",
    "\n",
    "\n",
    "def get_basal(root):\n",
    "    basal = []\n",
    "    basal_ts = []\n",
    "    for type_tag in root.findall('basal/event'):\n",
    "        value = type_tag.get('value')\n",
    "        ts = type_tag.get('ts')\n",
    "        ts = datetime.datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "        basal.append(float(value))\n",
    "        basal_ts.append(ts)\n",
    "        \n",
    "    basal_frame = [basal_ts, basal]\n",
    "    basal_frame = np.array(basal_frame)\n",
    "    df_basal = pd.DataFrame(basal_frame.T, columns=['ts', 'basal'])\n",
    "    return df_basal\n",
    "\n",
    "def get_bolus(root):\n",
    "    bolus = []\n",
    "    bolus_ts = []\n",
    "    for type_tag in root.findall('bolus_level/event'):\n",
    "        value = type_tag.get('value')\n",
    "        ts = type_tag.get('ts')\n",
    "        if ts is not None:\n",
    "            ts = datetime.datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "            bolus.append(float(value))\n",
    "            bolus_ts.append(ts)\n",
    "    bolus_frame = [bolus_ts, bolus]\n",
    "    bolus_frame = np.array(bolus_frame)\n",
    "    df_bolus = pd.DataFrame(bolus_frame.T, columns=['ts', 'bolus'])\n",
    "    df_bolus['bolus'] = df_bolus['bolus'].astype(float)\n",
    "    return df_bolus\n",
    "\n",
    "\n",
    "def get_temp_basal(root):\n",
    "    temp_basal = []\n",
    "    temp_basal_ts = []\n",
    "    for type_tag in root.findall('temp_basal/event'):\n",
    "        value = type_tag.get('value')\n",
    "        ts = type_tag.get('ts')\n",
    "        ts = datetime.datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "        temp_basal.append(float(value))\n",
    "        temp_basal_ts.append(ts)\n",
    "        \n",
    "    temp_basal_frame = [temp_basal_ts, temp_basal]\n",
    "    temp_basal_frame = np.array(temp_basal_frame)\n",
    "    df_temp_basal = pd.DataFrame(temp_basal_frame.T, columns=['ts', 'temp_basal'])\n",
    "    return df_temp_basal\n",
    "\n",
    "def get_macc(root):\n",
    "    macc = []\n",
    "    macc_ts = []\n",
    "    for type_tag in root.findall('macc/event'):\n",
    "        value = type_tag.get('value')\n",
    "        ts = type_tag.get('ts')\n",
    "        ts = datetime.datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "        macc.append(float(value))\n",
    "        macc_ts.append(ts)\n",
    "        \n",
    "    macc_frame = [macc_ts, macc]\n",
    "    macc_frame = np.array(macc_frame)\n",
    "    df_macc = pd.DataFrame(macc_frame.T, columns=['ts', 'macc'])\n",
    "    return df_macc\n",
    "\n",
    "def get_step(root):\n",
    "    step = []\n",
    "    step_ts = []\n",
    "    for type_tag in root.findall('step/event'):\n",
    "        value = type_tag.get('value')\n",
    "        ts = type_tag.get('ts')\n",
    "        ts = datetime.datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "        step.append(float(value))\n",
    "        step_ts.append(ts)\n",
    "        \n",
    "    step_frame = [step_ts, step]\n",
    "    step_frame = np.array(step_frame)\n",
    "    df_step = pd.DataFrame(step_frame.T, columns=['ts', 'step'])\n",
    "    return df_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f95b69c7-a3ba-4217-a73d-86fa29209fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function load the data, combines the single columns, fills in missing data and assings the classes\n",
    "# as input the file ordner which is either train or test, the subjects id, and finally the version which is 2018 or 2020 are given \n",
    "# linear interpolation and extrapolation are applied for missing values which are allowed to have a consecutive length of 2 hours\n",
    "\n",
    "\n",
    "def load_data(TRAINFILE, TESTFILE, s_ID, version):\n",
    "\n",
    "    count = 0\n",
    "    for i in range(0, len(TRAINFILE)):\n",
    "        root = ET.parse(TRAINFILE[i]).getroot()\n",
    "        root2 = ET.parse(TESTFILE[i]).getroot()\n",
    "\n",
    "        subject_ID = s_ID[count]\n",
    "        count = count +1\n",
    "        \n",
    "\n",
    "        # glucose, basal insulin, bolus insulin, and temp basal are stored as sepearte dataframes\n",
    "        df_glc = get_glc(root)\n",
    "        df_basal = get_basal(root)\n",
    "        df_bolus = get_bolus(root)\n",
    "        df_temp_basal = get_temp_basal(root)\n",
    "\n",
    "        # then the activity data is stored which calls either get_step() or get_macc() according to the chosen cohort\n",
    "        if version == 2018:\n",
    "            df_macc = get_step(root)\n",
    "        else:\n",
    "            df_macc = get_macc(root)\n",
    "\n",
    "        # the single dataframes are merged on the time and the subject id is added \n",
    "        df_list = [df_glc, df_basal, df_bolus, df_macc] \n",
    "        combined_df_train = df_list[0]\n",
    "        for i in range(1,len(df_list)):\n",
    "            combined_df_train = pd.merge(combined_df_train, df_list[i], on='ts', how='left')\n",
    "\n",
    "\n",
    "        # the same procedure is done for the test data\n",
    "        df_glc2 = get_glc(root2)\n",
    "        df_basal2 = get_basal(root2)\n",
    "        df_bolus2 = get_bolus(root2)\n",
    "        df_temp_basal2 = get_temp_basal(root2)\n",
    "\n",
    "        if version == 2018:\n",
    "            df_macc2 = get_step(root2)\n",
    "        else:\n",
    "            df_macc2 = get_macc(root2)\n",
    "\n",
    "        df_list2 = [df_glc2, df_basal2, df_bolus2, df_macc2] \n",
    "        combined_df_test = df_list2[0]\n",
    "        for i in range(1,len(df_list2)):\n",
    "            combined_df_test = pd.merge(combined_df_test, df_list2[i], on='ts', how='left')\n",
    "\n",
    "        # the train and test data are concatenated\n",
    "        combined_df = pd.concat([combined_df_train, combined_df_test])\n",
    "        combined_df[\"Subject_ID\"] = subject_ID\n",
    "        combined_df = combined_df.reset_index().drop(columns='index')\n",
    "\n",
    "        # the temporal basal replaces the original basal for the identified time intervalls of the train and then test files\n",
    "        combined_df = combine_basal_temp_basal(combined_df, df_temp_basal)\n",
    "        combined_df = combine_basal_temp_basal(combined_df, df_temp_basal2)\n",
    "\n",
    "        # the bolus insulin is integrated over the time interval on which it is applied and the the row bolus_end is deleted\n",
    "        for i in range (0, len(combined_df)):\n",
    "            if((combined_df[\"bolus\"][i]  != np.NaN)):\n",
    "                start_time = combined_df[\"ts\"][i]\n",
    "                end_time = combined_df[\"bolus_end\"][i]\n",
    "                combined_df.loc[(combined_df[\"ts\"] >= start_time) & (combined_df[\"ts\"] <= end_time), \"bolus\"] = combined_df[\"bolus\"][i]  \n",
    "        combined_df = combined_df.drop(\"bolus_end\", axis=1)\n",
    "\n",
    "        # the values are all converted to floats \n",
    "        combined_df['glucose'] = combined_df['glucose'].astype(str).astype(float)\n",
    "        combined_df['basal'] = combined_df['basal'].astype(str).astype(float)\n",
    "        combined_df['bolus'] = combined_df['bolus'].astype(str).astype(float)\n",
    "        combined_df['macc'] = combined_df['macc'].astype(str).astype(float)\n",
    "\n",
    "        # missing basal insulin is filled with the ffill and bfill method since the values are constantly infused\n",
    "        combined_df['basal'] = combined_df['basal'].fillna(method = 'ffill')\n",
    "        combined_df['basal'] = combined_df['basal'].fillna(method = 'bfill')\n",
    "        # missing bolus insulin is filled with 0 for nan values since most often missing values means that no bolus was infused\n",
    "        combined_df['bolus'] = combined_df['bolus'].fillna(0)\n",
    "\n",
    "\n",
    "        # the number of missing values for each parameter before data imputation is printed\n",
    "        print('Before Data Imputation')\n",
    "        print(subject_ID, 'intra:', combined_df.isna().sum())\n",
    "\n",
    "        # linear interpolation is applied for training data and linear extrapolation is applied for test data to fill some of the nan values in glucose and exercise data\n",
    "        combined_df2 = combined_df.copy()\n",
    "        # interpolation\n",
    "        combined_df = combined_df.interpolate(method = \"linear\", limit = 24, limit_direction=\"both\") \n",
    "        # extrapolation\n",
    "        combined_df2['glucose'] = combined_df2['glucose'].interpolate(method=\"slinear\", limit = 24, fill_value=\"extrapolate\", limit_direction=\"both\")\n",
    "        combined_df2['macc'] = combined_df2['macc'].interpolate(method=\"slinear\", limit = 24, fill_value=\"extrapolate\", limit_direction=\"both\")\n",
    "            \n",
    "\n",
    "        # remaining missing values in exercise data is filled with -1 indicating that no data was recorded\n",
    "        # those gaps were not removed, as glucose should be recorded continously to assign the classes, and they have the highest impact for the models\n",
    "        # and missing values could influence the performance significantly\n",
    "        # but it cannot be asserted that the patients will wear the wearable continously, as why the model should learn to ignore -1 values\n",
    "        combined_df['macc'] = combined_df['macc'].fillna(-1)\n",
    "        combined_df2['macc'] = combined_df2['macc'].fillna(-1)\n",
    "\n",
    "        # a column called Class is created and the value -1 is firstly assigned to each row to keep track of still available instances without a class\n",
    "        # then all glucose values below 70 mg/dL are given the Class 0\n",
    "        combined_df[\"Class\"] = -1\n",
    "        combined_df.loc[combined_df[\"glucose\"] <= 70, \"Class\"] = 0\n",
    "\n",
    "        # a list is created containing the timestamps of hypoglycemic events \n",
    "        list_hypo = (combined_df.loc[combined_df[\"Class\"] == 0, \"ts\"]).to_numpy()\n",
    "\n",
    "        # the function Class_generation() is called with wanted intervalls before a hypoglycemic event in minutes\n",
    "        combined_df = Class_generation(combined_df, 0, 15, 1, list_hypo) # 0-15\n",
    "        combined_df = Class_generation(combined_df, 15, 30, 2, list_hypo)  # 15-30 \n",
    "        combined_df = Class_generation(combined_df, 30, 60, 3, list_hypo)  # 30-60\n",
    "        combined_df = Class_generation(combined_df, 60, 120, 4, list_hypo)  # 1-2 \n",
    "        combined_df = Class_generation(combined_df, 120, 240, 5, list_hypo) # 2-4\n",
    "        combined_df = Class_generation(combined_df, 240, 480, 6, list_hypo)  # 4-8\n",
    "        combined_df = Class_generation(combined_df, 480, 720, 7, list_hypo)  # 8-12\n",
    "        combined_df = Class_generation(combined_df, 720, 1440, 8, list_hypo)  # 12-24\n",
    "        combined_df = Class_generation(combined_df, 1440, 2880, 9, list_hypo)  # 24-48\n",
    "        # 10 could be no hypoglycemia \n",
    "        combined_df.loc[combined_df[\"Class\"] == -1, \"Class\"] = 10\n",
    "\n",
    "        # same procedure is done for the extrapolated data \n",
    "        combined_df2[\"Class\"] = -1\n",
    "        combined_df2.loc[combined_df2[\"glucose\"] <= 70, \"Class\"] = 0\n",
    "\n",
    "        list_hypo_2 = (combined_df2.loc[combined_df2[\"Class\"] == 0, \"ts\"]).to_numpy()\n",
    "\n",
    "        combined_df2 = Class_generation(combined_df2, 0, 15, 1, list_hypo_2)  # 0-15\n",
    "        combined_df2 = Class_generation(combined_df2, 15, 30, 2, list_hypo_2)  # 15-30 \n",
    "        combined_df2 = Class_generation(combined_df2, 30, 60, 3, list_hypo_2)  # 30-60\n",
    "        combined_df2 = Class_generation(combined_df2, 60, 120, 4, list_hypo_2)  # 1-2 \n",
    "        combined_df2 = Class_generation(combined_df2, 120, 240, 5, list_hypo_2) # 2-4\n",
    "        combined_df2 = Class_generation(combined_df2, 240, 480, 6, list_hypo_2)  # 4-8\n",
    "        combined_df2 = Class_generation(combined_df2, 480, 720, 7, list_hypo_2)  # 8-12\n",
    "        combined_df2 = Class_generation(combined_df2, 720, 1440, 8, list_hypo_2)  # 12-24\n",
    "        combined_df2 = Class_generation(combined_df2, 1440, 2880, 9, list_hypo_2)  # 24-48\n",
    "        combined_df2.loc[combined_df2[\"Class\"] == -1, \"Class\"] = 10\n",
    "\n",
    "\n",
    "        # the number of missing values for each parameter before data imputation is printed\n",
    "        print('After Linear')\n",
    "        print(subject_ID, 'intra:', combined_df.isna().sum())\n",
    "        print(subject_ID, 'extra:', combined_df2.isna().sum())\n",
    "\n",
    "        \n",
    "        # the distribution of the classes is printed for interpolated and extrapolated data, respectively\n",
    "        print(np.bincount(combined_df['Class']))\n",
    "        print(len(combined_df['Class']))\n",
    "\n",
    "        print(np.bincount(combined_df2['Class']))\n",
    "        print(len(combined_df2['Class']))\n",
    "\n",
    "        # the function Remove_big_gaps() is called to identify consecutive nan values \n",
    "        # and to create subdataframes for each patient without any gaps, which are then saved as single csv files\n",
    "        Remove_big_gaps(combined_df, combined_df2, subject_ID, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5819e472-ad1a-45f4-9b08-b99d2f003488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function assigns the classes, it takes the start and end of the defined interval before the hypoglycemic event\n",
    "# furthermore, a list of all locations of hypoglycemic datapoints is given as input\n",
    "# only instances which were not assigned to another class are considered\n",
    "\n",
    "def Class_generation(df, start, end, class_number, list_hypo):\n",
    "    \n",
    "    # it is iterated over each hypoglycemic event and compute backwards with the given condition\n",
    "    for i in list_hypo:\n",
    "        current_time = pd.to_datetime(i)\n",
    "        start_time = current_time - datetime.timedelta(minutes = start)\n",
    "        end_time = current_time - datetime.timedelta(minutes = end)\n",
    "        # condition is checked, and the new class is assigned \n",
    "        df.loc[(df[\"ts\"] < start_time) & (df[\"ts\"] >= end_time) & (df[\"Class\"] == -1), \"Class\"] = class_number\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef62adf-4d50-4040-8b03-7f1e41ded458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function identifies gaps and split the dataframe into multiple dataframes which do not contain any missing values\n",
    "# as input data the interpolated and extrapoalted dataframes, the subject_ID, and the version of the cohort are given\n",
    "# (with the help of chatpgt)\n",
    "def Remove_big_gaps(df, df2, subject_ID, version):\n",
    "\n",
    "    df_inter = df.copy().reset_index()\n",
    "    dataframes_inter = []\n",
    "\n",
    "    # the indexes of nan values are identified to split the original data based on those gaps\n",
    "    nan_mask_inter = df_inter['glucose'].isnull()\n",
    "    # consecutive nan values are identified \n",
    "    cumultative_sum_inter = nan_mask_inter.cumsum()\n",
    "    # groups of consecutive nan values and non nan values are build\n",
    "    groups_inter = df_inter.groupby(cumultative_sum_inter)\n",
    "\n",
    "    # it is iterated through the groups and only the dataframes are added to the list which do not contain nan values\n",
    "    for _, group in groups_inter: \n",
    "        if group['glucose'].isnull().all(): \n",
    "            continue\n",
    "        group = group.dropna()\n",
    "        dataframes_inter.append(group)\n",
    "\n",
    "    # each dataframe which does not contain any nan value is saved for the specific person\n",
    "    for i in range (0, len(dataframes_inter)):\n",
    "        file_name = \"GAPS_DATA/TRAIN/%s/%s_%i_%i_INTER.csv\" % (subject_ID,subject_ID, i, version)\n",
    "        dataframes_inter[i].to_csv(file_name)\n",
    "\n",
    "\n",
    "    # the same is also done for the extrapolated data\n",
    "        \n",
    "    df_extra = df2.copy().reset_index()\n",
    "    dataframes_extra = []\n",
    "\n",
    "    nan_mask_extra = df_extra['glucose'].isnull()\n",
    "    cumultative_sum_extra = nan_mask_extra.cumsum()\n",
    "    groups_extra = df_extra.groupby(cumultative_sum_extra)\n",
    "\n",
    "    for _, group in groups_extra: \n",
    "        if group['glucose'].isnull().all(): \n",
    "            continue\n",
    "        group = group.dropna()\n",
    "        dataframes_extra.append(group)\n",
    "\n",
    "    for i in range (0, len(dataframes_extra)):\n",
    "        file_name2 = \"GAPS_DATA/TEST/%s/%s_%i_%i_EXTRA.csv\" % (subject_ID,subject_ID, i, version)\n",
    "        dataframes_extra[i].to_csv(file_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f3a0357-92a7-4cd6-9ff6-4c30c5ddb66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_Initial_Hypo(TRAINFILE, TESTFILE, s_ID):\n",
    "\n",
    "    count = 0\n",
    "    for i in range(0, len(TRAINFILE)):\n",
    "        root = ET.parse(TRAINFILE[i]).getroot()\n",
    "        root2 = ET.parse(TESTFILE[i]).getroot()\n",
    "\n",
    "        subject_ID = s_ID[count]\n",
    "        count = count +1\n",
    "\n",
    "        glucose = []\n",
    "        glucose_ts = []\n",
    "        for type_tag in root.findall('glucose_level/event'):\n",
    "            value = type_tag.get('value')\n",
    "            ts = type_tag.get('ts')\n",
    "            ts = datetime.datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "            glucose.append(int(value))\n",
    "            glucose_ts.append(ts)\n",
    "            \n",
    "        glc_frame = [glucose_ts, glucose]\n",
    "        glc_frame = np.array(glc_frame)\n",
    "        df_glc = pd.DataFrame(glc_frame.T, columns=['ts', 'glucose'])\n",
    "\n",
    "\n",
    "        glucose2 = []\n",
    "        glucose_ts2 = []\n",
    "        for type_tag in root2.findall('glucose_level/event'):\n",
    "            value2 = type_tag.get('value')\n",
    "            ts2 = type_tag.get('ts')\n",
    "            ts2 = datetime.datetime.strptime(ts2, \"%d-%m-%Y %H:%M:%S\")\n",
    "            glucose2.append(int(value2))\n",
    "            glucose_ts2.append(ts2)\n",
    "            \n",
    "        glc_frame2 = [glucose_ts2, glucose2]\n",
    "        glc_frame2 = np.array(glc_frame2)\n",
    "        df_glc2 = pd.DataFrame(glc_frame2.T, columns=['ts', 'glucose'])\n",
    "\n",
    "        df_glc3 = pd.concat([df_glc, df_glc2])\n",
    "\n",
    "        df_glc3[\"Class\"] = 1\n",
    "        df_glc3.loc[df_glc3[\"glucose\"] <= 70, \"Class\"] = 0\n",
    "        print(subject_ID)\n",
    "        print(np.bincount(df_glc3['Class']))\n",
    "        print(len(df_glc3['Class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a498c2fd-ad55-48cf-965a-52c633f1fc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function load the data, combines the single columns, fills in missing data and assings the classes\n",
    "# as input the file ordner which is either train or test, the subjects id, and finally the version which is 2018 or 2020 are given \n",
    "# linear interpolation and extrapolation are applied for missing values which are allowed to have a consecutive length of 2 hours\n",
    "\n",
    "def load_data(TRAINFILE, TESTFILE, s_ID, version):\n",
    "\n",
    "    count = 0\n",
    "    for i in range(0, len(TRAINFILE)):\n",
    "        root = ET.parse(TRAINFILE[i]).getroot()\n",
    "        root2 = ET.parse(TESTFILE[i]).getroot()\n",
    "\n",
    "        subject_ID = s_ID[count]\n",
    "        count = count +1\n",
    "        \n",
    "\n",
    "        # glucose, basal insulin, bolus insulin, and temp basal are stored as sepearte dataframes\n",
    "        df_glc = get_glc(root)\n",
    "        df_basal = get_basal(root)\n",
    "        df_bolus = get_bolus(root)\n",
    "        df_temp_basal = get_temp_basal(root)\n",
    "\n",
    "        # then the activity data is stored which calls either get_step() or get_macc() according to the chosen cohort\n",
    "        if version == 2018:\n",
    "            df_macc = get_step(root)\n",
    "        else:\n",
    "            df_macc = get_macc(root)\n",
    "\n",
    "        # the single dataframes are merged on the time and the subject id is added \n",
    "        df_list = [df_glc, df_basal, df_bolus, df_macc] \n",
    "        combined_df_train = df_list[0]\n",
    "        for i in range(1,len(df_list)):\n",
    "            combined_df_train = pd.merge(combined_df_train, df_list[i], on='ts', how='left')\n",
    "\n",
    "\n",
    "        # the same procedure is done for the test data\n",
    "        df_glc2 = get_glc(root2)\n",
    "        df_basal2 = get_basal(root2)\n",
    "        df_bolus2 = get_bolus(root2)\n",
    "        df_temp_basal2 = get_temp_basal(root2)\n",
    "\n",
    "        if version == 2018:\n",
    "            df_macc2 = get_step(root2)\n",
    "        else:\n",
    "            df_macc2 = get_macc(root2)\n",
    "\n",
    "        df_list2 = [df_glc2, df_basal2, df_bolus2, df_macc2] \n",
    "        combined_df_test = df_list2[0]\n",
    "        for i in range(1,len(df_list2)):\n",
    "            combined_df_test = pd.merge(combined_df_test, df_list2[i], on='ts', how='left')\n",
    "\n",
    "        # the train and test data are concatenated\n",
    "        combined_df = pd.concat([combined_df_train, combined_df_test])\n",
    "        combined_df[\"Subject_ID\"] = subject_ID\n",
    "        combined_df = combined_df.reset_index().drop(columns='index')\n",
    "\n",
    "        # the temporal basal replaces the original basal for the identified time intervalls of the train and then test files\n",
    "        combined_df = combine_basal_temp_basal(combined_df, df_temp_basal)\n",
    "        combined_df = combine_basal_temp_basal(combined_df, df_temp_basal2)\n",
    "\n",
    "        # the bolus insulin is integrated over the time interval on which it is applied and the the row bolus_end is deleted\n",
    "        for i in range (0, len(combined_df)):\n",
    "            if((combined_df[\"bolus\"][i]  != np.NaN)):\n",
    "                start_time = combined_df[\"ts\"][i]\n",
    "                end_time = combined_df[\"bolus_end\"][i]\n",
    "                combined_df.loc[(combined_df[\"ts\"] >= start_time) & (combined_df[\"ts\"] <= end_time), \"bolus\"] = combined_df[\"bolus\"][i]  \n",
    "        combined_df = combined_df.drop(\"bolus_end\", axis=1)\n",
    "\n",
    "        # the values are all converted to floats \n",
    "        combined_df['glucose'] = combined_df['glucose'].astype(str).astype(float)\n",
    "        combined_df['basal'] = combined_df['basal'].astype(str).astype(float)\n",
    "        combined_df['bolus'] = combined_df['bolus'].astype(str).astype(float)\n",
    "        combined_df['macc'] = combined_df['macc'].astype(str).astype(float)\n",
    "\n",
    "        # missing basal insulin is filled with the ffill and bfill method since the values are constantly infused\n",
    "        combined_df['basal'] = combined_df['basal'].fillna(method = 'ffill')\n",
    "        combined_df['basal'] = combined_df['basal'].fillna(method = 'bfill')\n",
    "        # missing bolus insulin is filled with 0 for nan values since most often missing values means that no bolus was infused\n",
    "        combined_df['bolus'] = combined_df['bolus'].fillna(0)\n",
    "\n",
    "\n",
    "        # the number of missing values for each parameter before data imputation is printed\n",
    "        print('Before Data Imputation')\n",
    "        print(subject_ID, 'intra:', combined_df.isna().sum())\n",
    "\n",
    "        # linear interpolation is applied for training data and linear extrapolation is applied for test data to fill some of the nan values in glucose and exercise data\n",
    "        combined_df2 = combined_df.copy()\n",
    "        # interpolation\n",
    "        combined_df = combined_df.interpolate(method = \"linear\", limit = 24, limit_direction=\"both\") \n",
    "        # extrapolation\n",
    "        combined_df2['glucose'] = combined_df2['glucose'].interpolate(method=\"slinear\", limit = 24, fill_value=\"extrapolate\", limit_direction=\"both\")\n",
    "        combined_df2['macc'] = combined_df2['macc'].interpolate(method=\"slinear\", limit = 24, fill_value=\"extrapolate\", limit_direction=\"both\")\n",
    "            \n",
    "\n",
    "        # remaining missing values in exercise data is filled with -1 indicating that no data was recorded\n",
    "        # those gaps were not removed, as glucose should be recorded continously to assign the classes, and they have the highest impact for the models\n",
    "        # and missing values could influence the performance significantly\n",
    "        # but it cannot be asserted that the patients will wear the wearable continously, as why the model should learn to ignore -1 values\n",
    "        combined_df['macc'] = combined_df['macc'].fillna(-1)\n",
    "        combined_df2['macc'] = combined_df2['macc'].fillna(-1)\n",
    "\n",
    "        # a column called Class is created and the value -1 is firstly assigned to each row to keep track of still available instances without a class\n",
    "        # then all glucose values below 70 mg/dL are given the Class 0\n",
    "        combined_df[\"Class\"] = -1\n",
    "        combined_df.loc[combined_df[\"glucose\"] <= 70, \"Class\"] = 0\n",
    "\n",
    "        # a list is created containing the timestamps of hypoglycemic events \n",
    "        list_hypo = (combined_df.loc[combined_df[\"Class\"] == 0, \"ts\"]).to_numpy()\n",
    "\n",
    "        # the function Class_generation() is called with wanted intervalls before a hypoglycemic event in minutes\n",
    "        combined_df = Class_generation(combined_df, 0, 15, 1, list_hypo) # 0-15\n",
    "        combined_df = Class_generation(combined_df, 15, 30, 2, list_hypo)  # 15-30 \n",
    "        combined_df = Class_generation(combined_df, 30, 60, 3, list_hypo)  # 30-60\n",
    "        combined_df = Class_generation(combined_df, 60, 120, 4, list_hypo)  # 1-2 \n",
    "        combined_df = Class_generation(combined_df, 120, 240, 5, list_hypo) # 2-4\n",
    "        combined_df = Class_generation(combined_df, 240, 480, 6, list_hypo)  # 4-8\n",
    "        combined_df = Class_generation(combined_df, 480, 720, 7, list_hypo)  # 8-12\n",
    "        combined_df = Class_generation(combined_df, 720, 1440, 8, list_hypo)  # 12-24\n",
    "        combined_df = Class_generation(combined_df, 1440, 2880, 9, list_hypo)  # 24-48\n",
    "        # 10 could be no hypoglycemia \n",
    "        combined_df.loc[combined_df[\"Class\"] == -1, \"Class\"] = 10\n",
    "\n",
    "        # same procedure is done for the extrapolated data \n",
    "        combined_df2[\"Class\"] = -1\n",
    "        combined_df2.loc[combined_df2[\"glucose\"] <= 70, \"Class\"] = 0\n",
    "\n",
    "        list_hypo_2 = (combined_df2.loc[combined_df2[\"Class\"] == 0, \"ts\"]).to_numpy()\n",
    "\n",
    "        combined_df2 = Class_generation(combined_df2, 0, 15, 1, list_hypo_2)  # 0-15\n",
    "        combined_df2 = Class_generation(combined_df2, 15, 30, 2, list_hypo_2)  # 15-30 \n",
    "        combined_df2 = Class_generation(combined_df2, 30, 60, 3, list_hypo_2)  # 30-60\n",
    "        combined_df2 = Class_generation(combined_df2, 60, 120, 4, list_hypo_2)  # 1-2 \n",
    "        combined_df2 = Class_generation(combined_df2, 120, 240, 5, list_hypo_2) # 2-4\n",
    "        combined_df2 = Class_generation(combined_df2, 240, 480, 6, list_hypo_2)  # 4-8\n",
    "        combined_df2 = Class_generation(combined_df2, 480, 720, 7, list_hypo_2)  # 8-12\n",
    "        combined_df2 = Class_generation(combined_df2, 720, 1440, 8, list_hypo_2)  # 12-24\n",
    "        combined_df2 = Class_generation(combined_df2, 1440, 2880, 9, list_hypo_2)  # 24-48\n",
    "        combined_df2.loc[combined_df2[\"Class\"] == -1, \"Class\"] = 10\n",
    "\n",
    "\n",
    "        # the number of missing values for each parameter before data imputation is printed\n",
    "        print('After Linear')\n",
    "        print(subject_ID, 'intra:', combined_df.isna().sum())\n",
    "        print(subject_ID, 'extra:', combined_df2.isna().sum())\n",
    "\n",
    "        \n",
    "        # the distribution of the classes is printed for interpolated and extrapolated data, respectively\n",
    "        print(np.bincount(combined_df['Class']))\n",
    "        print(len(combined_df['Class']))\n",
    "\n",
    "        print(np.bincount(combined_df2['Class']))\n",
    "        print(len(combined_df2['Class']))\n",
    "\n",
    "        # the function Remove_big_gaps() is called to identify consecutive nan values \n",
    "        # and to create subdataframes for each patient without any gaps, which are then saved as single csv files\n",
    "        Remove_big_gaps(combined_df, combined_df2, subject_ID, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0057521e-efc6-499b-9197-19b4ab708a40",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "strptime() argument 1 must be str, not None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 52\u001b[0m\n\u001b[0;32m     47\u001b[0m         load_data(train_files, test_files, patient_index, version\u001b[38;5;241m=\u001b[39mv) \n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 52\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[10], line 47\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m     train_files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Praktikum AI Med/OhioT1DM/2020/train/540-ws-training.xml\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     30\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Praktikum AI Med/OhioT1DM/2020/train/544-ws-training.xml\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     31\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Praktikum AI Med/OhioT1DM/2020/train/552-ws-training.xml\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Praktikum AI Med/OhioT1DM/2020/train/596-ws-training.xml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     35\u001b[0m                 ]\n\u001b[0;32m     38\u001b[0m     test_files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Praktikum AI Med/OhioT1DM/2020/test/540-ws-testing.xml\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     39\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Praktikum AI Med/OhioT1DM/2020/test/544-ws-testing.xml\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     40\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Praktikum AI Med/OhioT1DM/2020/test/552-ws-testing.xml\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Praktikum AI Med/OhioT1DM/2020/test/596-ws-testing.xml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     44\u001b[0m                 ]\n\u001b[1;32m---> 47\u001b[0m load_data(train_files, test_files, patient_index, version\u001b[38;5;241m=\u001b[39mv)\n",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(TRAINFILE, TESTFILE, s_ID, version)\u001b[0m\n\u001b[0;32m     18\u001b[0m df_basal \u001b[38;5;241m=\u001b[39m get_basal(root)\n\u001b[0;32m     19\u001b[0m df_bolus \u001b[38;5;241m=\u001b[39m get_bolus(root)\n\u001b[1;32m---> 20\u001b[0m df_temp_basal \u001b[38;5;241m=\u001b[39m get_temp_basal(root)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# then the activity data is stored which calls either get_step() or get_macc() according to the chosen cohort\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2018\u001b[39m:\n",
      "Cell \u001b[1;32mIn[4], line 55\u001b[0m, in \u001b[0;36mget_temp_basal\u001b[1;34m(root)\u001b[0m\n\u001b[0;32m     53\u001b[0m value \u001b[38;5;241m=\u001b[39m type_tag\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     54\u001b[0m ts \u001b[38;5;241m=\u001b[39m type_tag\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mts\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m ts \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mstrptime(ts, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m temp_basal\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(value))\n\u001b[0;32m     57\u001b[0m temp_basal_ts\u001b[38;5;241m.\u001b[39mappend(ts)\n",
      "\u001b[1;31mTypeError\u001b[0m: strptime() argument 1 must be str, not None"
     ]
    }
   ],
   "source": [
    "# main function which contains the files with their corresponsing subject id, modus and version  \n",
    "# this function is highly influenced by the code of https://github.com/r-cui/GluPred/blob/master/preprocess/linker.py\n",
    "def main():\n",
    "    versions_arr = [2018, 2020]\n",
    "\n",
    "    for v in versions_arr:\n",
    "        # first the data of the 2018 is preprocess \n",
    "        if (v == 2018):\n",
    "            patient_index = [559, 563, 570, 575, 588, 591]\n",
    "            train_files = ['/Praktikum AI Med/OhioT1DM/2018/train/559-ws-training.xml', \n",
    "                        '/Praktikum AI Med/OhioT1DM/2018/train/563-ws-training.xml',\n",
    "                        '/Praktikum AI Med/OhioT1DM/2018/train/570-ws-training.xml',\n",
    "                        '/Praktikum AI Med/OhioT1DM/2018/train/575-ws-training.xml',\n",
    "                        '/Praktikum AI Med/OhioT1DM/2018/train/588-ws-training.xml',\n",
    "                        '/Praktikum AI Med/OhioT1DM/2018/train/591-ws-training.xml'\n",
    "                        ]\n",
    "\n",
    "\n",
    "            test_files = ['/Praktikum AI Med/OhioT1DM/2018/test/559-ws-testing.xml', \n",
    "                        '/Praktikum AI Med/OhioT1DM/2018/test/563-ws-testing.xml',\n",
    "                        '/Praktikum AI Med/OhioT1DM/2018/test/570-ws-testing.xml',\n",
    "                        '/Praktikum AI Med/OhioT1DM/2018/test/575-ws-testing.xml',\n",
    "                        '/Praktikum AI Med/OhioT1DM/2018/test/588-ws-testing.xml',\n",
    "                        '/Praktikum AI Med/OhioT1DM/2018/test/591-ws-testing.xml'\n",
    "                        ]\n",
    "        # second, the data of the 2020 is preprocess     \n",
    "        elif (v == 2020):\n",
    "            patient_index = [540, 544, 552, 567, 584, 596]\n",
    "            train_files = ['/Praktikum AI Med/OhioT1DM/2020/train/540-ws-training.xml', \n",
    "                        '/Praktikum AI Med/OhioT1DM/2020/train/544-ws-training.xml',\n",
    "                        '/Praktikum AI Med/OhioT1DM/2020/train/552-ws-training.xml',\n",
    "                        '/Praktikum AI Med/OhioT1DM/2020/train/567-ws-training.xml',\n",
    "                        '/Praktikum AI Med/OhioT1DM/2020/train/584-ws-training.xml',\n",
    "                        '/Praktikum AI Med/OhioT1DM/2020/train/596-ws-training.xml'\n",
    "                        ]\n",
    "\n",
    "\n",
    "            test_files = ['/Praktikum AI Med/OhioT1DM/2020/test/540-ws-testing.xml', \n",
    "                        '/Praktikum AI Med/OhioT1DM/2020/test/544-ws-testing.xml',\n",
    "                        '/Praktikum AI Med/OhioT1DM/2020/test/552-ws-testing.xml',\n",
    "                        '/Praktikum AI Med/OhioT1DM/2020/test/567-ws-testing.xml',\n",
    "                        '/Praktikum AI Med/OhioT1DM/2020/test/584-ws-testing.xml',\n",
    "                        '/Praktikum AI Med/OhioT1DM/2020/test/596-ws-testing.xml'\n",
    "                        ]\n",
    "\n",
    "                \n",
    "        load_data(train_files, test_files, patient_index, version=v) \n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d7d3c-fc2a-4de7-bfdc-6942b0f2ce76",
   "metadata": {},
   "source": [
    "# Importing the required libraries \n",
    "import xml.etree.ElementTree as Xet \n",
    "\n",
    "cols = [\"name\", \"phone\", \"email\", \"date\", \"country\"] \n",
    "rows = [] \n",
    "\n",
    "# Parsing the XML file \n",
    "xmlparse = Xet.parse('sample.xml') \n",
    "root = xmlparse.getroot() \n",
    "for i in root: \n",
    "\tname = i.find(\"name\").text \n",
    "\tphone = i.find(\"phone\").text \n",
    "\temail = i.find(\"email\").text \n",
    "\tdate = i.find(\"date\").text \n",
    "\tcountry = i.find(\"country\").text \n",
    "\n",
    "\trows.append({\"name\": name, \n",
    "\t\t\t\t\"phone\": phone, \n",
    "\t\t\t\t\"email\": email, \n",
    "\t\t\t\t\"date\": date, \n",
    "\t\t\t\t\"country\": country}) \n",
    "\n",
    "df = pd.DataFrame(rows, columns=cols) \n",
    "\n",
    "# Writing dataframe to csv \n",
    "df.to_csv('output.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b60e687-a754-4ee6-8096-a37e4943e493",
   "metadata": {},
   "source": [
    "glucose_level {}\n",
    "finger_stick {}\n",
    "basal {}\n",
    "temp_basal {}\n",
    "bolus {}\n",
    "meal {}\n",
    "sleep {}\n",
    "work {}\n",
    "stressors {}\n",
    "hypo_event {}\n",
    "illness {}\n",
    "exercise {}\n",
    "basis_heart_rate {}\n",
    "basis_gsr {}\n",
    "basis_skin_temperature {}\n",
    "basis_air_temperature {}\n",
    "basis_steps {}\n",
    "basis_sleep {}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
